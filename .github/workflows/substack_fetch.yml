name: Fetch Substack Blog Posts
on:
  schedule: # Run workflow automatically
    - cron: '0 0 * * *' # Runs once a day at midnight
  workflow_dispatch: # Run workflow manually through the GitHub UI

jobs:
  fetch-substack-posts:
    name: Fetch latest blog posts from Substack
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests beautifulsoup4 python-dateutil requests-random-user-agent

      - name: Run script to fetch Substack posts
        uses: jannekem/run-python-script-action@v1
        with:
          script: |
            import feedparser
            import random
            import time
            import requests
            from datetime import datetime
            from requests_random_user_agent import RandomUserAgent

            def fetch_substack_feed(feed_url):
                """Fetch Substack feed with proxy rotation to avoid IP blocks"""
                
                # Try free proxy services (consider using a paid proxy service for production)
                free_proxy_lists = [
                    "https://free-proxy-list.net/",
                    "https://www.sslproxies.org/",
                    "https://www.us-proxy.org/"
                ]
                
                proxies = get_proxies(free_proxy_lists)
                
                # Set up headers with random user agent
                headers = {"User-Agent": RandomUserAgent().get_random_user_agent()}
                
                # First try without proxy
                try:
                    print("Attempting to fetch feed directly...")
                    response = requests.get(feed_url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        print("Direct fetch successful!")
                        return feedparser.parse(response.content)
                except Exception as e:
                    print(f"Direct fetch failed: {e}")
                
                # If direct fetch fails, try with proxies
                if proxies:
                    for proxy in proxies:
                        try:
                            print(f"Trying proxy: {proxy}")
                            proxy_dict = {
                                "http": f"http://{proxy}",
                                "https": f"http://{proxy}"
                            }
                            response = requests.get(feed_url, headers=headers, proxies=proxy_dict, timeout=15)
                            if response.status_code == 200:
                                print(f"Proxy fetch successful with {proxy}")
                                return feedparser.parse(response.content)
                        except Exception as e:
                            print(f"Proxy fetch failed with {proxy}: {e}")
                        
                        # Add delay between requests
                        time.sleep(1)
                
                # Fallback: use a public RSS proxy service
                print("Trying fallback method with RSS proxy...")
                try:
                    # Using a public RSS-to-JSON service as proxy
                    rss_proxy_url = f"https://api.rss2json.com/v1/api.json?rss_url={feed_url}"
                    response = requests.get(rss_proxy_url, headers=headers, timeout=15)
                    if response.status_code == 200:
                        print("Fallback method successful!")
                        json_data = response.json()
                        
                        # Convert the JSON response to format similar to feedparser
                        feed_data = {"entries": []}
                        if "items" in json_data:
                            for item in json_data["items"]:
                                feed_data["entries"].append({
                                    "title": item.get("title", ""),
                                    "link": item.get("link", ""),
                                    "published": item.get("pubDate", ""),
                                    "summary": item.get("description", "")
                                })
                        return feed_data
                    else:
                        print(f"Fallback method failed with status code: {response.status_code}")
                except Exception as e:
                    print(f"Fallback method failed: {e}")
                
                # Final fallback: try with Tor
                try:
                    print("Trying with Tor circuit...")
                    # This requires tor to be installed and running
                    tor_proxy = {
                        'http': 'socks5h://localhost:9050',
                        'https': 'socks5h://localhost:9050'
                    }
                    response = requests.get(feed_url, headers=headers, proxies=tor_proxy, timeout=30)
                    if response.status_code == 200:
                        print("Tor fetch successful!")
                        return feedparser.parse(response.content)
                except Exception as e:
                    print(f"Tor fetch failed: {e}")
                
                # If all methods fail, return empty structure
                print("All fetch methods failed!")
                return {"entries": []}

            def get_proxies(proxy_list_urls):
                """Get a list of free proxies from various sources"""
                import re
                from bs4 import BeautifulSoup
                
                all_proxies = []
                headers = {"User-Agent": RandomUserAgent().get_random_user_agent()}
                
                for url in proxy_list_urls:
                    try:
                        response = requests.get(url, headers=headers, timeout=10)
                        if response.status_code == 200:
                            soup = BeautifulSoup(response.text, 'html.parser')
                            
                            # Common pattern across many proxy list sites
                            table = soup.find('table')
                            if table:
                                for row in table.find_all('tr'):
                                    cells = row.find_all('td')
                                    if len(cells) >= 2:
                                        ip = cells[0].text.strip()
                                        port = cells[1].text.strip()
                                        if re.match(r'\d+\.\d+\.\d+\.\d+', ip) and port.isdigit():
                                            all_proxies.append(f"{ip}:{port}")
                    except Exception as e:
                        print(f"Error fetching proxies from {url}: {e}")
                
                # Randomize and limit list
                random.shuffle(all_proxies)
                return all_proxies[:10]  # Limit to 10 proxies to try
            
            # Main execution
            print("Starting Substack blog post fetcher...")
            
            # Your Substack feed URL
            FEED_URL = "https://datacommons.substack.com/feed"
            
            # Fetch the feed
            feed_data = fetch_substack_feed(FEED_URL)
            
            # Print the latest 5 posts
            if feed_data and "entries" in feed_data and feed_data["entries"]:
                print("\n========= LATEST 5 SUBSTACK BLOG POSTS =========\n")
                
                for i, entry in enumerate(feed_data["entries"][:5]):
                    print(f"{i+1}. {entry.get('title', 'No title')}")
                    print(f"   URL: {entry.get('link', 'No link')}")
                    print(f"   Published: {entry.get('published', 'No date')}")
                    print("")
                
                print("===============================================")
            else:
                print("No blog posts found or failed to fetch the feed.")

      - name: Show fetch results
        run: cat *.log || echo "No log file found"
